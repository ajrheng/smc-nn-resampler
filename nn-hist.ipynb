{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 10000\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "generate_data = False\n",
    "load = False\n",
    "save = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(n_input,75)\n",
    "        self.enc2 = nn.Linear(75,50)\n",
    "        self.enc3 = nn.Linear(50,25)\n",
    "        self.enc4 = nn.Linear(25,10)\n",
    "        \n",
    "        # decoder\n",
    "        self.dec1 = nn.Linear(10, 25)\n",
    "        self.dec2 = nn.Linear(25, 50)\n",
    "        self.dec3 = nn.Linear(50, 75)\n",
    "        self.dec4 = nn.Linear(75, n_output)\n",
    "        \n",
    "        #bn\n",
    "        self.bn_enc1 = nn.BatchNorm1d(75)\n",
    "        self.bn_enc2 = nn.BatchNorm1d(50)\n",
    "        self.bn_enc3 = nn.BatchNorm1d(25)\n",
    "        self.bn_enc4 = nn.BatchNorm1d(10)\n",
    "        self.bn_dec1 = nn.BatchNorm1d(25)\n",
    "        self.bn_dec2 = nn.BatchNorm1d(50)\n",
    "        self.bn_dec3 = nn.BatchNorm1d(75)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn_enc1(torch.relu(self.enc1(x)))\n",
    "        x = self.bn_enc2(torch.relu(self.enc2(x)))\n",
    "        x = self.bn_enc3(torch.relu(self.enc3(x)))\n",
    "        x = self.bn_enc4(torch.relu(self.enc4(x)))\n",
    "        x = self.bn_dec1(torch.relu(self.dec1(x)))\n",
    "        x = self.bn_dec2(torch.relu(self.dec2(x)))\n",
    "        x = self.bn_dec3(torch.relu(self.dec3(x)))\n",
    "        x = F.softmax(self.dec4(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_samples_per_mode(num_modes, total_points):\n",
    "    \"\"\"\n",
    "    Returns the number of points to sample for each number of mode.\n",
    "    \n",
    "    Args:\n",
    "        num_modes: number of different modes in the multimodal distribution\n",
    "        total_points: total number of points to draw for all modes\n",
    "        \n",
    "    Returns:\n",
    "        array with the number of samples per mode. the elements sum to total_points\n",
    "    \"\"\"\n",
    "    prob_per_mode = np.random.dirichlet(np.ones(num_modes))\n",
    "    num_samples = []\n",
    "    \n",
    "    for i in range(num_modes):\n",
    "        \n",
    "        if i == num_modes - 1: # if the last mode:\n",
    "            num = total_points - sum(num_samples)\n",
    "        else:\n",
    "            num = int(prob_per_mode[i] * total_points)\n",
    "            \n",
    "        num_samples.append(num)\n",
    "        \n",
    "    return np.array(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_binned_gaussian(num_data, num_samples, num_bins, num_modes):\n",
    "    \"\"\"\n",
    "    Returns (binned_data, binned_edges)\n",
    "    binned_data contains the binned training data, has dimensions [num_data, num_bins]\n",
    "    binned_edges contains the binned edges, has dimensions [num_data, num_bins+1]\n",
    "    \n",
    "    Args:\n",
    "        num_data: number of data points (full batch size)\n",
    "        num_samples: number of samples/particles to draw\n",
    "        num_bins: number of bins\n",
    "        num_modes: number of modes you want in the data. each data\n",
    "                   point will have a random number of modes from 1 to num_modes\n",
    "    \"\"\"\n",
    "    \n",
    "    binned_data = np.empty((0, num_bins), float)\n",
    "    edges = np.empty((0, num_bins+1), float)\n",
    "    \n",
    "    for i in tqdm(range(num_data)):\n",
    "        num_mode = np.random.randint(low=1, high=num_modes)\n",
    "        num_samples_list = num_samples_per_mode(num_mode, num_samples)\n",
    "        \n",
    "        points = np.empty(shape=0)\n",
    "        \n",
    "        for n_samples in num_samples_list:\n",
    "            mean = np.random.uniform(-10,10)\n",
    "            std = np.random.uniform(1,1.5)\n",
    "            points_ = np.random.normal(loc=mean, scale=std, size=n_samples)\n",
    "            points = np.append(points, points_)\n",
    "            \n",
    "        ## normalize\n",
    "        points =  (points - np.mean(points))/np.std(points)\n",
    "        bins_, edges_ = np.histogram(points, bins=num_bins)\n",
    "        bins_ = bins_/num_samples\n",
    "        \n",
    "        binned_data = np.append(binned_data, [bins_], axis=0)\n",
    "        edges = np.append(edges, [edges_], axis=0)\n",
    "        \n",
    "    return binned_data, edges\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_num_data = 50000\n",
    "test_num_data = 5000\n",
    "num_samples = 10000\n",
    "num_bins = 100\n",
    "num_modes = 10\n",
    "\n",
    "if generate_data is True:\n",
    "    train_data, train_edges = draw_binned_gaussian(train_num_data, num_samples, num_bins, num_modes)\n",
    "    test_data, test_edges = draw_binned_gaussian(test_num_data, num_samples, num_bins, num_modes)\n",
    "    np.save(\"train_data.npy\", train_data)\n",
    "    np.save(\"train_edges.npy\", train_edges)\n",
    "    np.save(\"test_data.npy\", test_data)\n",
    "    np.save(\"test_edges.npy\", test_edges)\n",
    "else:\n",
    "    train_data = np.load(\"train_data.npy\")\n",
    "    train_edges = np.load(\"train_edges.npy\")\n",
    "    test_data = np.load(\"test_data.npy\")\n",
    "    test_edges = np.load(\"test_edges.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.astype(np.float32)\n",
    "train_data = torch.from_numpy(train_data)\n",
    "# train_data = train_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.astype(np.float32)\n",
    "test_data = torch.from_numpy(test_data)\n",
    "test_data = test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(train_data)\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (enc1): Linear(in_features=100, out_features=75, bias=True)\n",
       "  (enc2): Linear(in_features=75, out_features=50, bias=True)\n",
       "  (enc3): Linear(in_features=50, out_features=25, bias=True)\n",
       "  (enc4): Linear(in_features=25, out_features=10, bias=True)\n",
       "  (dec1): Linear(in_features=10, out_features=25, bias=True)\n",
       "  (dec2): Linear(in_features=25, out_features=50, bias=True)\n",
       "  (dec3): Linear(in_features=50, out_features=75, bias=True)\n",
       "  (dec4): Linear(in_features=75, out_features=100, bias=True)\n",
       "  (bn_enc1): BatchNorm1d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_enc2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_enc3): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_enc4): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_dec1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_dec2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_dec3): BatchNorm1d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 1000\n",
    "net = Net(100,100)\n",
    "net = net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=250, gamma=0.3)\n",
    "loss_func = nn.KLDivLoss()\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_est_data = np.load(\"train_data.npy\")\n",
    "phase_est_data = phase_est_data.astype(np.float32)\n",
    "phase_est_data = torch.from_numpy(phase_est_data)\n",
    "phase_est_data = phase_est_data.to(device)\n",
    "\n",
    "phase_est_edges = np.load(\"train_edges.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, train loss: 0.000340, test loss: 0.000322, phase est loss: 0.000317\n",
      "Epoch: 9, train loss: 0.000209, test loss: 0.000211, phase est loss: 0.000207\n",
      "Epoch: 14, train loss: 0.000183, test loss: 0.000175, phase est loss: 0.000172\n",
      "Epoch: 19, train loss: 0.000173, test loss: 0.000185, phase est loss: 0.000182\n",
      "Epoch: 24, train loss: 0.000168, test loss: 0.000167, phase est loss: 0.000164\n",
      "Epoch: 29, train loss: 0.000165, test loss: 0.000153, phase est loss: 0.000150\n",
      "Epoch: 34, train loss: 0.000148, test loss: 0.000144, phase est loss: 0.000141\n",
      "Epoch: 39, train loss: 0.000128, test loss: 0.000115, phase est loss: 0.000112\n",
      "Epoch: 44, train loss: 0.000123, test loss: 0.000111, phase est loss: 0.000109\n",
      "Epoch: 49, train loss: 0.000118, test loss: 0.000109, phase est loss: 0.000107\n",
      "Epoch: 54, train loss: 0.000104, test loss: 0.000134, phase est loss: 0.000133\n",
      "Epoch: 59, train loss: 0.000103, test loss: 0.000094, phase est loss: 0.000092\n",
      "Epoch: 64, train loss: 0.000099, test loss: 0.000086, phase est loss: 0.000085\n",
      "Epoch: 69, train loss: 0.000095, test loss: 0.000100, phase est loss: 0.000098\n",
      "Epoch: 74, train loss: 0.000095, test loss: 0.000088, phase est loss: 0.000086\n",
      "Epoch: 79, train loss: 0.000097, test loss: 0.000086, phase est loss: 0.000085\n",
      "Epoch: 84, train loss: 0.000095, test loss: 0.000082, phase est loss: 0.000080\n",
      "Epoch: 89, train loss: 0.000092, test loss: 0.000090, phase est loss: 0.000088\n",
      "Epoch: 94, train loss: 0.000094, test loss: 0.000118, phase est loss: 0.000117\n",
      "Epoch: 99, train loss: 0.000091, test loss: 0.000083, phase est loss: 0.000082\n",
      "Epoch: 104, train loss: 0.000091, test loss: 0.000081, phase est loss: 0.000080\n",
      "Epoch: 109, train loss: 0.000090, test loss: 0.000079, phase est loss: 0.000078\n",
      "Epoch: 114, train loss: 0.000091, test loss: 0.000084, phase est loss: 0.000083\n",
      "Epoch: 119, train loss: 0.000088, test loss: 0.000081, phase est loss: 0.000080\n",
      "Epoch: 124, train loss: 0.000089, test loss: 0.000085, phase est loss: 0.000083\n",
      "Epoch: 129, train loss: 0.000089, test loss: 0.000080, phase est loss: 0.000079\n",
      "Epoch: 134, train loss: 0.000088, test loss: 0.000080, phase est loss: 0.000078\n",
      "Epoch: 139, train loss: 0.000087, test loss: 0.000079, phase est loss: 0.000078\n",
      "Epoch: 144, train loss: 0.000085, test loss: 0.000077, phase est loss: 0.000076\n",
      "Epoch: 149, train loss: 0.000087, test loss: 0.000085, phase est loss: 0.000084\n",
      "Epoch: 154, train loss: 0.000087, test loss: 0.000086, phase est loss: 0.000084\n",
      "Epoch: 159, train loss: 0.000085, test loss: 0.000077, phase est loss: 0.000076\n",
      "Epoch: 164, train loss: 0.000086, test loss: 0.000076, phase est loss: 0.000075\n",
      "Epoch: 169, train loss: 0.000086, test loss: 0.000083, phase est loss: 0.000082\n",
      "Epoch: 174, train loss: 0.000084, test loss: 0.000078, phase est loss: 0.000077\n",
      "Epoch: 179, train loss: 0.000083, test loss: 0.000077, phase est loss: 0.000076\n",
      "Epoch: 184, train loss: 0.000090, test loss: 0.000082, phase est loss: 0.000081\n",
      "Epoch: 189, train loss: 0.000085, test loss: 0.000076, phase est loss: 0.000075\n",
      "Epoch: 194, train loss: 0.000085, test loss: 0.000076, phase est loss: 0.000075\n",
      "Epoch: 199, train loss: 0.000084, test loss: 0.000075, phase est loss: 0.000074\n",
      "Epoch: 204, train loss: 0.000084, test loss: 0.000082, phase est loss: 0.000081\n",
      "Epoch: 209, train loss: 0.000083, test loss: 0.000074, phase est loss: 0.000073\n",
      "Epoch: 214, train loss: 0.000083, test loss: 0.000074, phase est loss: 0.000073\n",
      "Epoch: 219, train loss: 0.000078, test loss: 0.000069, phase est loss: 0.000068\n",
      "Epoch: 224, train loss: 0.000087, test loss: 0.000074, phase est loss: 0.000073\n",
      "Epoch: 229, train loss: 0.000076, test loss: 0.000070, phase est loss: 0.000069\n",
      "Epoch: 234, train loss: 0.000075, test loss: 0.000069, phase est loss: 0.000068\n",
      "Epoch: 239, train loss: 0.000077, test loss: 0.000067, phase est loss: 0.000066\n",
      "Epoch: 244, train loss: 0.000074, test loss: 0.000065, phase est loss: 0.000063\n",
      "Epoch: 249, train loss: 0.000074, test loss: 0.000065, phase est loss: 0.000064\n",
      "Epoch: 254, train loss: 0.000072, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 259, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000062\n",
      "Epoch: 264, train loss: 0.000071, test loss: 0.000063, phase est loss: 0.000062\n",
      "Epoch: 269, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 274, train loss: 0.000071, test loss: 0.000063, phase est loss: 0.000062\n",
      "Epoch: 279, train loss: 0.000071, test loss: 0.000063, phase est loss: 0.000063\n",
      "Epoch: 284, train loss: 0.000071, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 289, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 294, train loss: 0.000069, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 299, train loss: 0.000069, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 304, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 309, train loss: 0.000069, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 314, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 319, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 324, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 329, train loss: 0.000069, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 334, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000062\n",
      "Epoch: 339, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 344, train loss: 0.000069, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 349, train loss: 0.000069, test loss: 0.000068, phase est loss: 0.000067\n",
      "Epoch: 354, train loss: 0.000069, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 359, train loss: 0.000069, test loss: 0.000061, phase est loss: 0.000061\n",
      "Epoch: 364, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 369, train loss: 0.000069, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 374, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 379, train loss: 0.000070, test loss: 0.000064, phase est loss: 0.000063\n",
      "Epoch: 384, train loss: 0.000069, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 389, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 394, train loss: 0.000069, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 399, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 404, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 409, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 414, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 419, train loss: 0.000075, test loss: 0.000066, phase est loss: 0.000065\n",
      "Epoch: 424, train loss: 0.000068, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 429, train loss: 0.000069, test loss: 0.000061, phase est loss: 0.000061\n",
      "Epoch: 434, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 439, train loss: 0.000068, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 444, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000061\n",
      "Epoch: 449, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 454, train loss: 0.000069, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 459, train loss: 0.000069, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 464, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 469, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 474, train loss: 0.000070, test loss: 0.000063, phase est loss: 0.000062\n",
      "Epoch: 479, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 484, train loss: 0.000070, test loss: 0.000062, phase est loss: 0.000061\n",
      "Epoch: 489, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 494, train loss: 0.000068, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 499, train loss: 0.000069, test loss: 0.000065, phase est loss: 0.000064\n",
      "Epoch: 504, train loss: 0.000068, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 509, train loss: 0.000067, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 514, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 519, train loss: 0.000068, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 524, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 529, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 534, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 539, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 544, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 549, train loss: 0.000067, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 554, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 559, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 564, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 569, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 574, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 579, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 584, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 589, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 594, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 599, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 604, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 609, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 614, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 619, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 624, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 629, train loss: 0.000067, test loss: 0.000064, phase est loss: 0.000063\n",
      "Epoch: 634, train loss: 0.000066, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 639, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 644, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 649, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000060\n",
      "Epoch: 654, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 659, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 664, train loss: 0.000067, test loss: 0.000061, phase est loss: 0.000060\n",
      "Epoch: 669, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000060\n",
      "Epoch: 674, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 679, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 684, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 689, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 694, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 699, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 704, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 709, train loss: 0.000066, test loss: 0.000059, phase est loss: 0.000059\n",
      "Epoch: 714, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 719, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 724, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 729, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 734, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 739, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 744, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 749, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 754, train loss: 0.000065, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 759, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 764, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 769, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 774, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 779, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 784, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 789, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 794, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 799, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 804, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 809, train loss: 0.000065, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 814, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 819, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 824, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 829, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 834, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 839, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 844, train loss: 0.000065, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 849, train loss: 0.000065, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 854, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 859, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 864, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 869, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 874, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 879, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 884, train loss: 0.000066, test loss: 0.000059, phase est loss: 0.000059\n",
      "Epoch: 889, train loss: 0.000065, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 894, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 899, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 904, train loss: 0.000067, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 909, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 914, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 919, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 924, train loss: 0.000066, test loss: 0.000059, phase est loss: 0.000059\n",
      "Epoch: 929, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 934, train loss: 0.000065, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 939, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 944, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 949, train loss: 0.000066, test loss: 0.000059, phase est loss: 0.000058\n",
      "Epoch: 954, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 959, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 964, train loss: 0.000065, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 969, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 974, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 979, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 984, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 989, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 994, train loss: 0.000066, test loss: 0.000060, phase est loss: 0.000059\n",
      "Epoch: 999, train loss: 0.000067, test loss: 0.000059, phase est loss: 0.000059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (enc1): Linear(in_features=100, out_features=75, bias=True)\n",
       "  (enc2): Linear(in_features=75, out_features=50, bias=True)\n",
       "  (enc3): Linear(in_features=50, out_features=25, bias=True)\n",
       "  (enc4): Linear(in_features=25, out_features=10, bias=True)\n",
       "  (dec1): Linear(in_features=10, out_features=25, bias=True)\n",
       "  (dec2): Linear(in_features=25, out_features=50, bias=True)\n",
       "  (dec3): Linear(in_features=50, out_features=75, bias=True)\n",
       "  (dec4): Linear(in_features=75, out_features=100, bias=True)\n",
       "  (bn_enc1): BatchNorm1d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_enc2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_enc3): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_enc4): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_dec1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_dec2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_dec3): BatchNorm1d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if load is False:\n",
    "    for e in range(epochs):\n",
    "        train_loss_cum_sum = 0\n",
    "        iters_per_epoch = 0\n",
    "        for train_data_ in train_data_loader:\n",
    "            train_data_ = train_data_.to(device)\n",
    "            prediction = net(train_data_)\n",
    "            loss = loss_func(torch.log(prediction), train_data_)\n",
    "            train_loss = loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_cum_sum += train_loss\n",
    "            iters_per_epoch += 1\n",
    "        \n",
    "\n",
    "        if (e+1)%5 == 0:\n",
    "            net.eval()\n",
    "            test_pred = net(test_data)\n",
    "            loss = loss_func(torch.log(test_pred), test_data)\n",
    "            test_loss = loss.item()\n",
    "\n",
    "            phase_est_pred = net(phase_est_data)\n",
    "            loss = loss_func(torch.log(phase_est_pred), phase_est_data)\n",
    "            phase_est_loss = loss.item()\n",
    "\n",
    "            print(\"Epoch: {:d}, train loss: {:f}, test loss: {:f}, phase est loss: {:f}\"\n",
    "                  .format(e, train_loss_cum_sum/iters_per_epoch, test_loss, phase_est_loss))\n",
    "            net.train()\n",
    "                \n",
    "        scheduler.step()\n",
    "        \n",
    "    if save is True:\n",
    "        torch.save(net.state_dict(), \"net_bn_aft_relu.model\")\n",
    "else:\n",
    "    net.load_state_dict(torch.load(\"net_bn_aft_relu.model\"))\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pred = net(test_data)\n",
    "idx_to_plot = np.random.choice(test_num_data, size=100)\n",
    "\n",
    "for i in idx_to_plot:\n",
    "    \n",
    "    test_edges_ = test_edges[i][:-1]\n",
    "    edge_width = test_edges_[1] - test_edges_[0]\n",
    "\n",
    "    plt.bar(test_edges_, test_data[i].cpu(), align='edge', width = edge_width)\n",
    "    plt.bar(test_edges_, test_pred[i].detach().cpu(), align='edge', width = edge_width, alpha=0.7)\n",
    "    plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phase_est_pred = net(phase_est_data)\n",
    "idx_to_plot = np.random.choice(len(phase_est_data), size=100)\n",
    "\n",
    "for i in idx_to_plot:\n",
    "    \n",
    "    phase_est_edges_ = phase_est_edges[i][:-1]\n",
    "    edge_width = phase_est_edges_[1] - phase_est_edges_[0]\n",
    "\n",
    "    plt.bar(phase_est_edges_, phase_est_data[i].cpu(), align='edge', width = edge_width)\n",
    "    plt.bar(phase_est_edges_, phase_est_pred[i].detach().cpu(), align='edge', width = edge_width, alpha=0.7)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
